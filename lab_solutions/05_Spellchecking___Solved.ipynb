{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA2yps58_R1C"
      },
      "source": [
        "# 1. Sorri not veri gud in inglish\n",
        "\n",
        "Have you ever googled someone's name without knowing exactly how it should be written? Were you ever reluctant to look up the correct spelling of a query you typed? Or just unable to type properly because of being in a rush? Modern search engines usually do a pretty good job in deciphering defective user input. In order to be able to do that, a good spell-checking mechanism should be incorporated into a search procedure. Today we will take one step further towards building a good search engine and work on tolerant retrieval with respect to user queries. We will consider two cases:\n",
        "\n",
        "1. Users know that they don't know the correct spelling OR they want to get the results that follow some known pattern, so the use so called wildcards - queries like `retr*val`;\n",
        "2. Users don't know the correct spelling OR they don't care OR they are in a rush OR they expect that mistakes will be corrected OR /your option/... so they make mistakes and we need to handle them using:\n",
        "\n",
        "    2.1. Trigrams with Jaccard coefficient;\n",
        "    \n",
        "    2.2. Simple spellchecker by Peter Norvig with QWERTY weights;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4_pUzhs_R1J"
      },
      "source": [
        "## 1.1. Handling wildcards\n",
        "\n",
        "We will handle wildcard queries using k-grams. K-gram is a list of consecutive k chars in a string - i.e., for the word *'star'*, it will be '*\\$st*', '*sta*', '*tar*', and '*ar$*', if we take k=3. Take a look at the [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)'s *chapter 3.2.2* to understand how k-grams can help efficiently match a wildcard against dictionary words. Here we will only consider wildcards with star symbols (may be multiple).\n",
        "\n",
        "Notice that for building k-grams index, **we will need a vocabulary of original (correct) word forms** to compare words in user input to the vocabulary of \"correct\" words (think why inverted index which we built for stemmed words doesn't work here).   \n",
        "\n",
        "You need to implement the following:\n",
        "\n",
        "- `build_inverted_index_orig_forms` - creates inverted index of original world forms from `facts` list, which is already given to you.  \n",
        "    Output format: `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
        "    \n",
        "\n",
        "- `build_k_gram_index` - creates k-gram index which maps every k-gram encountered in facts collection to a list of words containing this k-gram. Use the abovementioned inverted index of original words to construct this index.  \n",
        "    Output format: `'k_gram': ['word1_with_k_gram', 'word2_with_k_gram', ...]`\n",
        "    \n",
        "    \n",
        "- `generate_wildcard_options` - produce a list of vocabulary words matching given wildcard by intersecting postings of k-grams present in the wildcard (refer to *ch 3.2.2*). \n",
        "\n",
        "- `search_wildcard` - return list of facts that contain the words matching a wildcard query.\n",
        "\n",
        "\n",
        "We will use the dataset with curious facts for testing.\n",
        "\n",
        "### 1.1.1. Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go9LMYrB_R1K",
        "outputId": "90ba9a91-e829-437c-f5d7-dbc109736dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
            "152. There are more stars in space than there are grains of sand on every beach in the world.\n",
            "153. For every human on Earth there are 1.6 million ants.\n",
            "154. The total weight of all those ants, however, is about the same as all the humans.\n",
            "155. On Jupiter and Saturn it rains diamonds.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "data_url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
        "local_filename, headers = urllib.request.urlretrieve(data_url)\n",
        "\n",
        "facts = []\n",
        "with open(local_filename) as fp:\n",
        "    for cnt, line in enumerate(fp):\n",
        "        facts.append(line.strip('\\n'))\n",
        "        \n",
        "print(*facts[-5:], sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kn1IETJ_R1N"
      },
      "source": [
        "### 1.1.2. Implementation of search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iud0BXNz_R1N"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def build_inverted_index_orig_forms(documents):\n",
        "    inverted_index = {}\n",
        "    \n",
        "    for doc_id, doc in enumerate(documents):\n",
        "        for word in set(nltk.word_tokenize(doc.lower())):\n",
        "            if word.isalpha():\n",
        "                if word not in inverted_index:\n",
        "                    inverted_index[word] = [1, (doc_id, 1)]\n",
        "                else:\n",
        "                    inverted_index[word][0] += 1\n",
        "                    inverted_index[word].append((doc_id, 1))\n",
        "    \n",
        "    return inverted_index\n",
        "\n",
        "def build_k_gram_index(inverted_index, k):\n",
        "    k_gram_index = {}\n",
        "    \n",
        "    for word in inverted_index.keys():\n",
        "        padded_word = f\"${word}$\"\n",
        "        for i in range(len(padded_word) - k + 1):\n",
        "            k_gram = padded_word[i:i+k]\n",
        "            if k_gram not in k_gram_index:\n",
        "                k_gram_index[k_gram] = [word]\n",
        "            else:\n",
        "                k_gram_index[k_gram].append(word)\n",
        "    \n",
        "    return k_gram_index\n",
        "\n",
        "def generate_wildcard_options(wildcard, k_gram_index, inverted_index):\n",
        "    padded_wildcard = f\"${wildcard}$\"\n",
        "    wildcard_k_grams = [padded_wildcard[i:i+3] for i in range(len(padded_wildcard) - 2)]\n",
        "    candidate_sets = [set(k_gram_index[k_gram]) for k_gram in wildcard_k_grams if k_gram in k_gram_index]\n",
        "\n",
        "    if not candidate_sets:\n",
        "        return []\n",
        "\n",
        "    common_words = set.intersection(*candidate_sets)\n",
        "    regex = re.compile(\"^\" + wildcard.replace(\"*\", \".*\") + \"$\")\n",
        "    matching_words = [word for word in common_words if regex.match(word)]\n",
        "    \n",
        "    return matching_words\n",
        "\n",
        "def search_wildcard(wildcard, k_gram_index, index, docs):\n",
        "    wildcard_options = generate_wildcard_options(wildcard, k_gram_index, index)\n",
        "    regex = re.compile(r'\\b(?:' + '|'.join(wildcard_options) + r')\\b', flags=re.IGNORECASE)\n",
        "    results = [doc for doc in docs if regex.search(doc)]\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split_str_3gram = lambda s, k=3: list(map(''.join, zip(*[iter(s)]*k)))\n",
        "\n",
        "n = 3\n",
        "your_string=  \"$wild$\"\n",
        "[your_string[i:i+n] for i in range(0, len(your_string)-2)]\n",
        "\n",
        "# split_str_3gram(\"$wild$\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ONV-DoDiAp",
        "outputId": "50250405-eadd-408f-846b-4a79c9fac839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$wi', 'wil', 'ild', 'ld$']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_orig_forms = build_inverted_index_orig_forms(facts)\n",
        "k_gram_index = build_k_gram_index(index_orig_forms, 3)\n",
        "k_gram_index['$th']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTQoXvgEE6CN",
        "outputId": "77cd4cd6-f3ea-47e6-f32c-545809531fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'their',\n",
              " 'than',\n",
              " 'there',\n",
              " 'that',\n",
              " 'these',\n",
              " 'they',\n",
              " 'them',\n",
              " 'three',\n",
              " 'things',\n",
              " 'thigh',\n",
              " 'thought',\n",
              " 'this',\n",
              " 'themselves',\n",
              " 'thirsty',\n",
              " 'those']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4SouwKs_R1O"
      },
      "source": [
        "### 1.1.3. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC6YCIol_R1P",
        "outputId": "8a0cd83e-637c-480c-b014-4d47750aae73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reduced', 'recorded', 'received']\n",
            "4. The largest \u001b[1m\u001b[91mrecorded\u001b[0m snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
            "102. More than 50% of the people in the world have never made or \u001b[1m\u001b[91mreceived\u001b[0m a telephone call.\n",
            "134. A person can live without food for about a month, but only about a week without water. If the amount of water in your body is \u001b[1m\u001b[91mreduced\u001b[0m by just 1%, you'll feel thirsty. If it's \u001b[1m\u001b[91mreduced\u001b[0m by 10%, you'll die.\n"
          ]
        }
      ],
      "source": [
        "index_orig_forms = build_inverted_index_orig_forms(facts)\n",
        "k_gram_index = build_k_gram_index(index_orig_forms, 3)\n",
        "\n",
        "wildcard = \"re*ed\"\n",
        "\n",
        "wildcard_options = generate_wildcard_options(wildcard, k_gram_index, index_orig_forms)\n",
        "print(wildcard_options)\n",
        "assert(len(wildcard_options) >= 3)\n",
        "assert(\"red\" not in wildcard_options) \n",
        "\n",
        "wildcard_results = search_wildcard(wildcard, k_gram_index, index_orig_forms, facts)\n",
        "# some pretty printing\n",
        "for r in wildcard_results:\n",
        "    # highlight terms for visual evaluation\n",
        "    for term in wildcard_options:\n",
        "        r = re.sub(r'(' + term + ')', r'\\033[1m\\033[91m\\1\\033[0m', r, flags=re.I)\n",
        "    print(r)\n",
        "\n",
        "assert(len(wildcard_results) >=3)\n",
        "\n",
        "assert \"13. James Buchanan, the 15th U.S. president continuously bought slaves with his own money in order to free them.\" in search_wildcard(\"pres*dent\", k_gram_index, index_orig_forms, facts)\n",
        "assert \"40. 9 out of 10 Americans are deficient in Potassium.\" in search_wildcard(\"p*tas*um\", k_gram_index, index_orig_forms, facts)\n",
        "assert \"61. A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\" in search_wildcard(\"*price\", k_gram_index, index_orig_forms, facts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNona3QU_R1P"
      },
      "source": [
        "## 1.2. Handling typos\n",
        "\n",
        "### 1.2.0. Dataset \n",
        "\n",
        "Download github typo dataset from [here](https://github.com/mhagiwara/github-typo-corpus).\n",
        "Load it with this code:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM0CH2F--4Hj",
        "outputId": "b74a5022-8df4-4515-f2d6-8ca84607a9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 20:09:16--  https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
            "Resolving github-typo-corpus.s3.amazonaws.com (github-typo-corpus.s3.amazonaws.com)... 3.5.25.191, 52.217.192.49, 52.217.47.124, ...\n",
            "Connecting to github-typo-corpus.s3.amazonaws.com (github-typo-corpus.s3.amazonaws.com)|3.5.25.191|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43769081 (42M) [application/x-gzip]\n",
            "Saving to: ‘github-typo-corpus.v1.0.0.jsonl.gz’\n",
            "\n",
            "github-typo-corpus. 100%[===================>]  41.74M  71.3MB/s    in 0.6s    \n",
            "\n",
            "2023-05-08 20:09:17 (71.3 MB/s) - ‘github-typo-corpus.v1.0.0.jsonl.gz’ saved [43769081/43769081]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -xvzf github-typo-corpus.v1.0.0.jsonl.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fov2G66A-49K",
        "outputId": "f569a5bd-6fff-4ea2-d72d-75c40a7247e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: This does not look like a tar archive\n",
            "tar: Skipping to next header\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip github-typo-corpus.v1.0.0.jsonl.gz\n"
      ],
      "metadata": {
        "id": "Qo9KRaA9_Exj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3SUdNOt_R1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17b27e1-f542-4928-c3ae-e765aedda837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBV_NLKk_R1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2737eb27-16c9-4592-f1a6-7e264caf7ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size = 245909\n"
          ]
        }
      ],
      "source": [
        "import jsonlines\n",
        "\n",
        "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
        "\n",
        "dataset = []\n",
        "other_langs = set()\n",
        "\n",
        "with jsonlines.open(dataset_file) as reader:\n",
        "    for obj in reader:\n",
        "        for edit in obj['edits']:\n",
        "            if edit['src']['lang'] != 'eng':\n",
        "                other_langs.add(edit['src']['lang'])\n",
        "                continue\n",
        "\n",
        "            if edit['is_typo']:\n",
        "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
        "                if src.lower() != tgt.lower():\n",
        "                    dataset.append((edit['src']['text'], edit['tgt']['text']))\n",
        "                \n",
        "print(f\"Dataset size = {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAwzGYFn_R1R"
      },
      "source": [
        "#### Explore sample typos\n",
        "Please, explore the dataset. You may see, that this is\n",
        "- mostly markdown\n",
        "- some common mistakes with do/does\n",
        "- some just refer to punctuation typos (which we do not consider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTOoqywr_R1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b99e10f-a72c-48b7-fab1-947837d3c1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        \"\"\"Make am instance. =>         \"\"\"Make an instance.\n",
            "* travis: test agains Node.js 11 => * travis: test against Node.js 11\n",
            "The parser receive a string and returns an array inside a user-provided  => The parser receives a string and returns an array inside a user-provided \n",
            "CSV data is send through the `write` function and the resulted data is obtained => CSV data is sent through the `write` function and the resulting data is obtained\n",
            "One useful function part of the Stream API is `pipe` to interact between  => One useful function of the Stream API is `pipe` to interact between \n",
            "source to a `stream.Writable` object destination. This example available as  => source to a `stream.Writable` object destination. This example is available as \n",
            "`node samples/pipe.js` read the file, parse its content and transform it. => `node samples/pipe.js` and reads the file, parses its content and transforms it.\n",
            "Most of the generator is imported from its parent project [CSV][csv] in a effort  => Most of the generator is imported from its parent project [CSV][csv] in an effort \n",
            "*   `quote`             Optionnal character surrounding a field, one character only, defaults to double quotes.    => *   `quote`             Optional character surrounding a field, one character only, defaults to double quotes.   \n",
            "The parser receive a string and return an array inside a user-provided  => The parser receive a string and returns an array inside a user-provided \n"
          ]
        }
      ],
      "source": [
        "for pair in dataset[1010:1020]:\n",
        "    print(f\"{pair[0]} => {pair[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vahMHU5k_R1S"
      },
      "source": [
        "#### 1.2.0.1. Build a dataset vocabulary\n",
        "Here we prepare a vocabulary for spellchecker testing and for estimating overall correction quality. Consider only word-level. Be carefull, there is markdown (e.g. \\`name\\`. \\[url\\]\\(http://url)) and comment symbols (\\#, //, \\*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7NDipH__R1S"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sent):\n",
        "    # splits sentence to words, filtering out non-alphabetical terms\n",
        "    words = nltk.word_tokenize(sent)    \n",
        "    words_filtered = filter(lambda x: x.isalpha(), words)\n",
        "    return words_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiREM0qF_R1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5c61fd-ad2c-4257-a386-80af6376b506"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63724"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from collections import Counter\n",
        "vocabulary = Counter()\n",
        "for pair in dataset:\n",
        "    for word in sent_to_words(pair[1].lower()):\n",
        "        vocabulary[word] += 1\n",
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e6T0biH_R1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394afdbf-274e-4f91-d971-d3db572c3f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('function', 6193), ('de', 82), ('deutsch', 4), ('nocomments', 2), ('you', 42075), ('can', 26027), ('disable', 532), ('comments', 360), ('for', 44756), ('the', 207017)]\n"
          ]
        }
      ],
      "source": [
        "from itertools import islice\n",
        "print(list(islice(vocabulary.items(), 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sZUOaGr_R1T"
      },
      "source": [
        "### 1.2.1. Implement context-independent spellcheker (Trigrams with Jaccard coefficient) ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PttcMAJ_R1T"
      },
      "outputs": [],
      "source": [
        "\n",
        "def jaccard_index(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0\n",
        "\n",
        "def fix_typo_kgram(word, k_gram_index, k=3, top_n=5):\n",
        "    padded_word = f\"${word}$\"\n",
        "    word_k_grams = [padded_word[i:i+k] for i in range(len(padded_word) - k + 1)]\n",
        "    word_k_grams_set = set(word_k_grams)\n",
        "    \n",
        "    candidate_words = set()\n",
        "    for k_gram in word_k_grams:\n",
        "        if k_gram in k_gram_index:\n",
        "            candidate_words.update(k_gram_index[k_gram])\n",
        "    \n",
        "    jaccard_scores = [(candidate, jaccard_index(word_k_grams_set, set([f\"${candidate}$\"[i:i+k] for i in range(len(candidate) - k + 2)]))) for candidate in candidate_words]\n",
        "    jaccard_scores.sort(key=lambda x: -x[1])\n",
        "    \n",
        "    best_matches = [word_score_pair[0] for word_score_pair in jaccard_scores[:top_n]]\n",
        "    \n",
        "    return best_matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owNkABax_R1T"
      },
      "source": [
        "### 1.2.2. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VJeDU_M_R1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78612c18-a480-4b94-8cf4-74bd1647ffa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['enough', 'eno', 'enought', 'endogenous', 'enoent']\n"
          ]
        }
      ],
      "source": [
        "# tests\n",
        "\n",
        "k_gram_index_github = build_k_gram_index(vocabulary, 3)\n",
        "print(fix_typo_kgram(\"enouh\", k_gram_index_github)[:20])\n",
        "assert \"enough\" in fix_typo_kgram(\"enouh\", k_gram_index_github), \"Assert k-gram failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIqAZZ_y_R1U"
      },
      "source": [
        "## 1.3. [Extra tasks, for fun]\n",
        "\n",
        "### 1.3.1. QWERTY - Editorial distance\n",
        "\n",
        "Write the code to compute weighted QWERTY-editorial distance.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/KB_United_States.svg/640px-KB_United_States.svg.png\" width=\"640\"/> \n",
        "\n",
        "Use this image to prepare weight function:\n",
        "- all letter pairs which share vertical border will get 0.5 multiplier **for replace** (`df`, `cv`, `ui`, ...)\n",
        "- all letter pairs which share at least some horizontal border will get 0.7 multiplier **for replace** (`dc`, `dr`, `km`, ...)\n",
        "- other operations are not scaled (x1 multiplier)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwerty = [\n",
        "    \"1234567890\",\n",
        "    \"qwertyuiop\",\n",
        "    \"asdfghjkl\",\n",
        "    \"zxcvbnm\"\n",
        "]\n",
        "\n",
        "keyboard_positions = {}\n",
        "for row, keys in enumerate(qwerty):\n",
        "    for col, key in enumerate(keys):\n",
        "        keyboard_positions[key] = (row, col)\n"
      ],
      "metadata": {
        "id": "VRpigN7e_5s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_weight(let1, let2):\n",
        "    qwerty_rows = [\"1234567890\", \"qwertyuiop\", \"asdfghjkl\", \"zxcvbnm\"]\n",
        "    row_multiplier = [0.5, 0.7]\n",
        "    \n",
        "    for i, row in enumerate(qwerty_rows):\n",
        "        if let1 in row and let2 in row:\n",
        "            if abs(row.index(let1) - row.index(let2)) == 1:\n",
        "                return row_multiplier[i % 2]\n",
        "        elif (let1 in qwerty_rows[1] and let2 in qwerty_rows[2]) or (let1 in qwerty_rows[2] and let2 in qwerty_rows[1]):\n",
        "            let1_index = row.index(let1) if let1 in row else None\n",
        "            let2_index = row.index(let2) if let2 in row else None\n",
        "            if let1_index is not None and let2_index is not None:\n",
        "                if abs(let1_index - let2_index) <= 1:\n",
        "                    return 0.7\n",
        "    return 1.0\n",
        "\n",
        "\n",
        "def qwerty_edit_dist(s1, s2):\n",
        "    len1 = len(s1)\n",
        "    len2 = len(s2)\n",
        "    d = [[0.0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]\n",
        "\n",
        "    for i in range(len1 + 1):\n",
        "        d[i][0] = i\n",
        "\n",
        "    for j in range(len2 + 1):\n",
        "        d[0][j] = j\n",
        "\n",
        "    for i in range(1, len1 + 1):\n",
        "        for j in range(1, len2 + 1):\n",
        "            cost = replace_weight(s1[i - 1], s2[j - 1]) if s1[i - 1] != s2[j - 1] else 0.0\n",
        "            d[i][j] = min(\n",
        "                d[i - 1][j] + 1,  # deletion\n",
        "                d[i][j - 1] + 1,  # insertion\n",
        "                d[i - 1][j - 1] + cost  # substitution\n",
        "            )\n",
        "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
        "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + 1)  # transposition\n",
        "\n",
        "    return d[len1][len2]\n",
        "\n",
        "# tests\n",
        "assert qwerty_edit_dist(\"korrectud\", \"corrected\") == 2.0, \"Edit distance is computed incorrectly\"\n",
        "assert qwerty_edit_dist(\"soem\", \"some\") == 1.0, \"Edit distance is computed incorrectly\"\n",
        "assert qwerty_edit_dist(\"one\", \"one\") == 0.0, \"Edit distance is computed incorrectly\"\n",
        "assert qwerty_edit_dist(\"fihure\", \"figure\") == 0.5, \"Edit distance is computed incorrectly\"\n",
        "assert qwerty_edit_dist(\"fivue\", \"figure\") == 0.7, \"Edit distance is computed incorrectly\"\n"
      ],
      "metadata": {
        "id": "V7jaFjhC_-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45_VWeI_R1V"
      },
      "source": [
        "### 1.3.2. Norvig's spellchecker with QWERTY weights\n",
        "\n",
        "You can base your code on [Norvig's corrector](https://norvig.com/spell-correct.html), but you should be sure you account the fact, that typos for close letters cost less. This should be considered in ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uBEy04K_R1V"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(\"your large corpus of text goes here\"))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "  res = WORDS[word] / N\n",
        "  return 0.01 if res==0 else res\n",
        "\n",
        "def candidates(word): \n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def fix_typo_qwerty_norvig(word):\n",
        "    return min(candidates(word), key=lambda w: qwerty_edit_dist(word, w) / P(w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktoPhGjf_R1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "4791f54a-fc3f-4f12-9e40-1a29e5747154"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-3d8ea24c6353>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mfix_typo_qwerty_norvig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"korrectud\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"corrected\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Norvig's correcter doesn't work\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mfix_typo_qwerty_norvig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"speling\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spelling\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Norvig's correcter doesn't work\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mfix_typo_qwerty_norvig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condidence\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"confidence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Norvig's correcter doesn't work\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-3f42b7957894>\u001b[0m in \u001b[0;36mfix_typo_qwerty_norvig\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfix_typo_qwerty_norvig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqwerty_edit_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# # tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-3f42b7957894>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfix_typo_qwerty_norvig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqwerty_edit_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# # tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ],
      "source": [
        "# tests\n",
        "\n",
        "assert fix_typo_qwerty_norvig(\"korrectud\") == \"corrected\", \"Norvig's correcter doesn't work\"\n",
        "assert fix_typo_qwerty_norvig(\"speling\") == \"spelling\", \"Norvig's correcter doesn't work\"\n",
        "assert fix_typo_qwerty_norvig(\"condidence\") == \"confidence\", \"Norvig's correcter doesn't work\"\n",
        "assert fix_typo_qwerty_norvig(\"fpx\") == \"fox\", \"Norvig's correcter doesn't work\"\n",
        "assert fix_typo_qwerty_norvig(\"fux\") == \"fix\", \"Norvig's correcter doesn't work\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axW9lL7U_R1W"
      },
      "source": [
        "### 12.3.3. Estimate quality of functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2HQOZ6H_R1W"
      },
      "outputs": [],
      "source": [
        "norvig, kgram = 0, 0\n",
        "limit = 10000\n",
        "counter = limit\n",
        "for i, (src, target) in enumerate(dataset):\n",
        "    if i == limit:\n",
        "        break\n",
        "    words = sent_to_words(src.lower())\n",
        "    # word suspected for typos\n",
        "    sn, sk = src.lower(), src.lower()\n",
        "    for word in words:\n",
        "        if word not in vocabulary and word.isalpha():\n",
        "            # top-1 accuracy\n",
        "            wn, wk = fix_typo_qwerty_norvig(word), \\\n",
        "                     fix_typo_kgram(word, k_gram_index_github)[0]\n",
        "            sn = sn.replace(word, wn)\n",
        "            sk = sk.replace(word, wk)\n",
        "    norvig += int(sn == target.lower())\n",
        "    kgram += int(sk == target.lower())\n",
        "\n",
        "print(f\"Norvig accuracy ({norvig}) = {norvig / limit}\")\n",
        "print(f\"k-gram accuracy ({kgram}) = {kgram / limit}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}